{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Exploring COVID-19 and dengue numbers: the data acquisition process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Environment log:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "(1) Using the \"analytics3\" anaconda environment; installing python packages directly through *pip* command.  \n",
    "(2) Installed *pyvirtualenv*, *selenium*, and *[bs4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#)* packages.  \n",
    "(3) Installed *chromedriver* through \"pip install chromedriver\".  \n",
    "\n",
    "(4) Got this error when initializing the Display function:\n",
    "> display = Display(visible=0, size=(800,600))\n",
    "> display.start()  \n",
    ">> FileNotFoundError: [Errno 2] No such file or directory: 'Xvfb': 'Xvfb'\n",
    ">> EasyProcessError: start error EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 2] No such file or directory: 'Xvfb': 'Xvfb' return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>\n",
    "\n",
    "**Solution found in: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=860501**   \n",
    " \n",
    "(5) Installed the system package xvfb through the shell line: *sudo apt-get install xvfb*.\n",
    "\n",
    "- When running the \"driver = webdriver.Chrome()\" python command, I received the following error:  \n",
    "> driver = webdriver.Chrome()\n",
    ">> FileNotFoundError: [Errno 2] No such file or directory: 'chromedriver': 'chromedriver'  \n",
    "\n",
    "(6) Even though, since I'm using conda environment I have set the chromedriver path before:  \n",
    "> chromeDriverPath = '~/anaconda3/envs/anaytics3/'\n",
    "\n",
    "The solution was to create a symbolic link in the path shown above, through the system bash, which made it work properly:  \n",
    "> $ ln -s ~/anaconda3/envs/analytics3/chromedriver-Linux64 chromedriver  \n",
    "\n",
    "(7) There are some broken links in the BMH oficial page: it misses the \"https://portalarquivos.saude.gov.br/\" portion of the link. We defined a function to address this issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Epidemiological bulletins from World Health Organization - WHO\n",
    "https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports/  \n",
    "https://github.com/danielsteman/COVID-19_WHO  \n",
    "https://github.com/danielsteman/COVID-19_WHO/blob/master/WHO_webscrape.ipynb (the author uses PyPDF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Epidemiological bulletins from Brazilian Ministry of Health\n",
    "https://www.saude.gov.br/boletins-epidemiologicos\n",
    "  - Coronavírus/COVID-19  \n",
    "  - Dengue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Initial statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pdb   # Python debugger\n",
    "from pyvirtualdisplay import Display\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Definição de parâmetros\n",
    "url = 'https://www.saude.gov.br/boletins-epidemiologicos'\n",
    "chromeDriverPath = '~/anaconda3/envs/anaytics3/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Web browsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this step of our data crawling we want to access the webpage through the Chromium driver. It is then appropriate to create some functions both for the page access as for its content analysis. However, it would be only possible if we know the page structure. In order to reach it, at this time we will try to read and explore it, trying to find the patterns and links we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '800x600x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '800x600x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display starting:\n",
    "display = Display(visible=0, size=(800,600))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Open the Chromium.driver for the intended page:\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reading the page content and saving it specifying its encoding scheme:\n",
    "driver.get(url)\n",
    "page = driver.page_source.encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490660\n",
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "# How large is the loaded page?\n",
    "print(len(page))\n",
    "# Data streaming type is expected:\n",
    "print(type(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html><!--[if lt IE 7]>      <html class=\"no-js lt-ie9 lt-ie8 lt-ie7\" lang=\"pt-br\" dir=\"ltr\"> <![endif]--><!--[if IE 7]>         <html class=\"no-js lt-ie9 lt-ie8\" lang=\"pt-br\" dir=\"ltr\"> <![endif]--><!--[if IE 8]>         <html class=\"no-js lt-ie9\" lang=\"pt-br\" dir=\"ltr\"> <![endif]--><!--[if gt IE 8]><!--><html xmlns=\"http://www.w3.org/1999/xhtml\" class=\"no-js\" lang=\"pt-br\" dir=\"ltr\"><!--<![endif]--><head>\\n<!-- Google Tag Manager -->\\n<script type=\"text/javascript\" async=\"\" src=\"https://www.google-analytics.com/gtm/js?id=GTM-W26S6PL&amp;t=gtm2&amp;cid=639199945.1588552376\"></script><script type=\"text/javascript\" async=\"\" src=\"https://www.google-analytics.com/analytics.js\"></script><script type=\"text/javascript\" async=\"\" src=\"https://www.google-analytics.com/analytics.js\"></script><script src=\"https://connect.facebook.net/pt_BR/sdk.js?hash=054ff8773cb77fd6b084be4b8df5b59e&amp;ua=modern_es6\" async=\"\" crossorigin=\"anonymous\"></script><script id=\"facebook-jssdk\" src=\"//connect.facebook.net/pt_BR/sdk.js#xfbml=1&amp;version=v2.10&amp;appId=343943812426295\"></script><script id=\"twitter-wjs\" src=\"https://platform.twitter.com/widgets.js\"></script><script async=\"\" src=\"https://www.googletagmanager.com/gtm.js?id=GTM-N9GV4MG\"></script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\\'gtm.start\\':\\nnew Date().getTime(),event:\\'gtm.js\\'});var f=d.getElementsByTagName(s)[0],\\nj=d.createElement(s),dl=l!=\\'dataLayer\\'?\\'&amp;l=\\'+l:\\'\\';j.async=true;j.src=\\n\\'https://www.googletagmanager.com/gtm.js?id=\\'+i+dl;f.parentNode.insertBefore(j,f);\\n})(window,document,\\'script\\',\\'dataLayer\\',\\'GTM-N9GV4MG\\');</script>\\n<!-- End Google Tag Manager -->\\n       <!--[if lt IE 9]>\\n    <script src=\"/templates/padraogoverno01/js/html5shiv.js\"></script>\\n    <![endif]-->\\n    <link rel=\"stylesheet\" href=\"/templates/padraogoverno01/css/custom.css?version=31\" type=\"text/css\" />\\n    <link rel=\"stylesheet\" href=\"/templates/padraogoverno01/css/custom2.css?version=18\" type=\"text/css\" />\\n    <link rel=\"stylesheet\" h'\n"
     ]
    }
   ],
   "source": [
    "## Showing only the first 2k positions of the bytes stream:\n",
    "print(page[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Since the loaded page has too many links, as well as other contents we are not interested in, we will use BeautifulSoap to parse it in order to obtain the desired links.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbnsilveira/anaconda3/envs/analytics3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/tbnsilveira/anaconda3/envs/analytics3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Aplicando o *parsing* na página:\n",
    "soup = BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Although every HTML file shares an equivalent structure, parsing a web page involves knowing how it was specifically built. If you are not the page developer, you should expect to spend some time at it, exploring the way the information you are looking for is arranged among all classes and divisions.  \n",
    "\n",
    "A helpful and simpler way to do this (at least from the point of view of a data scientist and not a web developer) is by making use of the Chrome inspection tool, as demonstrated in the image below:  \n",
    "![Inspecting the BMH web page with the Chrome browser tool.](\"BMH_Chrome_inspection.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Getting the data:  \n",
    "We are interested in the *Coronavírus/COVID-19* and *Dengue* data, which can be reached from the \"Por assunto\" (By subject) menu in the main page. Through some iterations, we can see that the links (what we are looking for) are inserted in a sibling class of the one that contains the reference title. For that, what we must do is search for the parent element of our title of interest and, from there, explore all the links that match our search criteria (link of the type \"Http\" for a file of the type PDF). The function below condenses this search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def BMH_findLinks_bySubject(subject, soup, interestStructure = 'h3'):\n",
    "    '''Given a subject of interest, look for the HTTP links available from the web page of the Brazilian Ministry of Health.\n",
    "    BMH page: https://www.saude.gov.br/boletins-epidemiologicos\n",
    "    Syntax: \n",
    "        subject: string type with the subject of interest. Check in the BMH page for the right spelling.\n",
    "            (e.g.: \"Coronavírus/COVID-19\" or \"Dengue\")\n",
    "        soup: BeautifulSoup data structure from the web page of interest.\n",
    "        interestStructure: the HTML type of the structure of interest. The default value is 'h3'. \n",
    "        \n",
    "    Returns:\n",
    "        links: a list of links corresponding to the searching criteria.\n",
    "    '''\n",
    "    ## Finding the section corresponding to the subject:\n",
    "    section = soup.find(interestStructure, string=subject)\n",
    "    \n",
    "    ## Finding the parent element:\n",
    "    section_parent = section.find_parent()\n",
    "    \n",
    "    ## Finding all the HTTP links in the parent section corresponding to a PDF file:\n",
    "    linkList = []\n",
    "    for link in section_parent.find_all('a', href=True):\n",
    "        if link['href'].lower().endswith(\".pdf\"):\n",
    "            if link['href'].lower().startswith(\"http\"):\n",
    "                linkList.append(link['href'])\n",
    "\n",
    "    return linkList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From the function above, we can now look for all epidemiological bulletins about *Coronavírus/COVID-19* from the scraped page of the Brazilian Ministry of Health:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Obtaining the list:\n",
    "links_COVID = BMH_findLinks_bySubject('Coronavírus/COVID-19', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/27/2020-04-27-18-05h-BEE14-Boletim-do-COE.pdf\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/21/BE13---Boletim-do-COE.pdf\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/19/BE12-Boletim-do-COE.pdf\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/17/2020-04-16---BE10---Boletim-do-COE-21h.pdf\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/12/2020-04-11-BE9-Boletim-do-COE.pdf\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/03/BE6-Boletim-Especial-do-COE.pdf\n",
      "https://portalarquivos2.saude.gov.br/images/pdf/2020/fevereiro/13/Boletim-epidemiologico-COEcorona-SVS-13fev20.pdf\n"
     ]
    }
   ],
   "source": [
    "## Checking out each item:\n",
    "for link in links_COVID:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### WARNING: broken links\n",
    "Always check if your scripts are retrieving the desired data. If we open the page of interest in a web browser, we would find there are more bulletins than those resulting from the query above. To understand why this is happening, we must go back to a step-by-step manual inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## (1) Looking for the section we are interested in the parsed page:\n",
    "section = soup.find('h3', string='Coronavírus/COVID-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## (2) Moving up to the parent section:\n",
    "section_parent = section.find_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"#\">Coronavírus/COVID-19</a>,\n",
       " <a href=\"https://portalarquivos.saude.gov.br/images/pdf/2020/April/27/2020-04-27-18-05h-BEE14-Boletim-do-COE.pdf\" rel=\"noopener\" target=\"_blank\">BE 14 - Boletim COE Coronavírus</a>,\n",
       " <a href=\"https://portalarquivos.saude.gov.br/images/pdf/2020/April/21/BE13---Boletim-do-COE.pdf\">BE 13 - Boletim COE Coronavírus</a>,\n",
       " <a href=\"https://portalarquivos.saude.gov.br/images/pdf/2020/April/19/BE12-Boletim-do-COE.pdf\">BE 12 - Boletim COE Coronavírus</a>,\n",
       " <a href=\"/images/pdf/2020/April/18/2020-04-17---BE11---Boletim-do-COE-21h.pdf\" target=\"_blank\">BE 11 - Boletim COE Coronavírus</a>,\n",
       " <a href=\"https://portalarquivos.saude.gov.br/images/pdf/2020/April/17/2020-04-16---BE10---Boletim-do-COE-21h.pdf\" rel=\"noopener\" target=\"_blank\">BE 10 - Boletim COE Coronavirus</a>,\n",
       " <a href=\"https://portalarquivos.saude.gov.br/images/pdf/2020/April/12/2020-04-11-BE9-Boletim-do-COE.pdf\" rel=\"noopener\" target=\"_blank\">BE9 - Boletim Especial do COE Coronavírus Avaliação de Risco</a>,\n",
       " <a href=\"/images/pdf/2020/April/09/be-covid-08-final-2.pdf\" target=\"_blank\">BE8 - Boletim Especial do COE Coronavírus Avaliação de Risco</a>,\n",
       " <a href=\"/images/pdf/2020/April/09/be-covid-08-final.pdf\" target=\"_blank\"></a>,\n",
       " <a href=\"/images/pdf/2020/April/06/2020-04-06-BE7-Boletim-Especial-do-COE-Atualizacao-da-Avaliacao-de-Risco.pdf\" target=\"_blank\">BE7 - Boletim Especial do COE Coronavírus Avaliação de Risco</a>,\n",
       " <a href=\"https://portalarquivos.saude.gov.br/images/pdf/2020/April/03/BE6-Boletim-Especial-do-COE.pdf\">BE6 - Boletim Especial do COE Coronavírus Avaliação de Risc 03/04/2020</a>,\n",
       " <a href=\"/images/pdf/2020/marco/21/2020-03-13-Boletim-Epidemiologico-05.pdf\" target=\"_blank\">BE Coronavírus - 13/03/2020</a>,\n",
       " <a href=\"/images/pdf/2020/marco/04/2020-03-02-Boletim-Epidemiol--gico-04-corrigido.pdf\">BE Coronavírus - 04/03/2020</a>,\n",
       " <a href=\"/images/pdf/2020/fevereiro/21/2020-02-21-Boletim-Epidemiologico03.pdf\" target=\"_blank\">BE Coronavírus - 21/02/2020</a>,\n",
       " <a href=\"https://portalarquivos2.saude.gov.br/images/pdf/2020/fevereiro/13/Boletim-epidemiologico-COEcorona-SVS-13fev20.pdf\" rel=\"noopener\" target=\"_blank\">BE Coronavírus - 10/02/2020</a>,\n",
       " <a href=\"/images/pdf/2020/fevereiro/18/Boletim-epidemiologico-SVS-01-COE-inundacao.pdf\">BE COE Inundação nº 01 - Gestão de Emergências em Saúde Pública por desastres: evento hidrometeorológico na região Sudeste do Brasil, 2020</a>,\n",
       " <a href=\"/images/pdf/2020/fevereiro/04/Boletim-epidemiologico-SVS-04fev20.pdf\">BE Coronavírus - 03/02/2020</a>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## (3) Checking manually what is going on:\n",
    "section_parent.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> We can always make use of visualization commands to turn things easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "['Coronavírus/COVID-19'] \n",
      "\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/27/2020-04-27-18-05h-BEE14-Boletim-do-COE.pdf\n",
      "['BE 14 - Boletim COE Coronavírus'] \n",
      "\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/21/BE13---Boletim-do-COE.pdf\n",
      "['BE 13 - Boletim COE Coronavírus'] \n",
      "\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/19/BE12-Boletim-do-COE.pdf\n",
      "['BE 12 - Boletim COE Coronavírus'] \n",
      "\n",
      "/images/pdf/2020/April/18/2020-04-17---BE11---Boletim-do-COE-21h.pdf\n",
      "['BE 11 - Boletim COE Coronavírus'] \n",
      "\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/17/2020-04-16---BE10---Boletim-do-COE-21h.pdf\n",
      "['BE 10 - Boletim COE Coronavirus'] \n",
      "\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/12/2020-04-11-BE9-Boletim-do-COE.pdf\n",
      "['BE9 - Boletim Especial do COE Coronavírus Avaliação de Risco'] \n",
      "\n",
      "/images/pdf/2020/April/09/be-covid-08-final-2.pdf\n",
      "['BE8 - Boletim Especial do COE Coronavírus Avaliação de Risco'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in section_parent.find_all('a')[:8]:  #Notice we are limiting the visualization for the first nine links\n",
    "    print(report['href'])\n",
    "    print(report.contents,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Bingo! **There are some broken links...** Observe the lines where instead of starting with \"https\", goes straightly to the directory structure. To address this issue, let's rewrite our previous function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def BMH_findLinks_bySubject_v2(subject, soup, rootPage='https://www.saude.gov.br', interestStructure ='h3'):\n",
    "    '''Given a subject of interest, look for the HTTP links available from the web page of the Brazilian Ministry of Health.\n",
    "    BMH page: https://www.saude.gov.br/boletins-epidemiologicos\n",
    "    Syntax: \n",
    "        subject: string type with the subject of interest. Check in the BMH page for the right spelling.\n",
    "            (e.g.: \"Coronavírus/COVID-19\" or \"Dengue\")\n",
    "        soup: BeautifulSoup data structure from the web page of interest.\n",
    "        interestStructure: the HTML type of the structure of interest. The default value is 'h3'.\n",
    "        rootPage: in the case there are broken links, i.e., those starting with a directory structure \n",
    "            instead of a \"http\" statement, this address will be joint to them.\n",
    "            E.g.: 'https://www.saude.gov.br' or 'https://portalarquivos.saude.gov.br'\n",
    "        \n",
    "    Returns:\n",
    "        links: a list of links corresponding to the searching criteria.\n",
    "    '''\n",
    "    ## Finding the section corresponding to the subject:\n",
    "    section = soup.find(interestStructure, string=subject)\n",
    "    \n",
    "    ## Finding the parent element:\n",
    "    section_parent = section.find_parent()\n",
    "    \n",
    "    ## Finding all the HTTP links in the parent section corresponding to a PDF file:\n",
    "    linkList = []\n",
    "    for link in section_parent.find_all('a', href=True):\n",
    "        if link['href'].lower().endswith(\".pdf\"):\n",
    "            if link['href'].lower().startswith(\"http\"):\n",
    "                linkList.append(link['href'])\n",
    "\n",
    "            ## Adding the root page to the broken links:\n",
    "            elif link['href'].lower().startswith(\"/images/\"):\n",
    "                linkList.append(rootPage+link['href'])\n",
    "\n",
    "    return linkList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "-----------------------------------_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Testing, once again, the function we created to retrieve the bulletins about COVID-19 from BMH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Obtaining the list:\n",
    "links_COVID = BMH_findLinks_bySubject_v2('Coronavírus/COVID-19', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/27/2020-04-27-18-05h-BEE14-Boletim-do-COE.pdf\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/21/BE13---Boletim-do-COE.pdf\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/19/BE12-Boletim-do-COE.pdf\n",
      "https://www.saude.gov.br/images/pdf/2020/April/18/2020-04-17---BE11---Boletim-do-COE-21h.pdf\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/17/2020-04-16---BE10---Boletim-do-COE-21h.pdf\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/12/2020-04-11-BE9-Boletim-do-COE.pdf\n",
      "https://www.saude.gov.br/images/pdf/2020/April/09/be-covid-08-final-2.pdf\n",
      "https://www.saude.gov.br/images/pdf/2020/April/09/be-covid-08-final.pdf\n",
      "https://www.saude.gov.br/images/pdf/2020/April/06/2020-04-06-BE7-Boletim-Especial-do-COE-Atualizacao-da-Avaliacao-de-Risco.pdf\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/03/BE6-Boletim-Especial-do-COE.pdf\n",
      "https://www.saude.gov.br/images/pdf/2020/marco/21/2020-03-13-Boletim-Epidemiologico-05.pdf\n",
      "https://www.saude.gov.br/images/pdf/2020/marco/04/2020-03-02-Boletim-Epidemiol--gico-04-corrigido.pdf\n",
      "https://www.saude.gov.br/images/pdf/2020/fevereiro/21/2020-02-21-Boletim-Epidemiologico03.pdf\n",
      "https://portalarquivos2.saude.gov.br/images/pdf/2020/fevereiro/13/Boletim-epidemiologico-COEcorona-SVS-13fev20.pdf\n",
      "https://www.saude.gov.br/images/pdf/2020/fevereiro/18/Boletim-epidemiologico-SVS-01-COE-inundacao.pdf\n",
      "https://www.saude.gov.br/images/pdf/2020/fevereiro/04/Boletim-epidemiologico-SVS-04fev20.pdf\n"
     ]
    }
   ],
   "source": [
    "## Checking out each item:\n",
    "for link in links_COVID:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since there are not so many links, one may try to audit it from the original web page. Doing so, one would find out there is an aditional link here. If we pay attention, there are two files regarding the 8th bulleting, probably a second version due to formating issues. From a data science perspective, it is not a problem at all since such data overlap must be carried out during the data processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--------------------------_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Just for checking, we can also try the same functions for other diseases, as for example the \"Asbestose\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "links_asbestose = BMH_findLinks_bySubject_v2('Asbestose', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.saude.gov.br/images/pdf/2016/fevereiro/02/2015-011---Asbestose.pdf\n"
     ]
    }
   ],
   "source": [
    "## Checking out each item:\n",
    "for link in links_asbestose:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Downloading the files - Version 1.0 [There's an updated version below]\n",
    "At first, someone could use the os.system library ('wget http ...') to download each file. However, in order to allow the code portability, it is recommended to use the own Python libraries whenever it is possible. \n",
    "[Ref.: http://stackoverflow.com/questions/2467609/using-wget-via-python]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using the current version of the urllib library (*request* module) does not allow one to specify the destination folder. Based on [this reference](https://stackoverflow.com/questions/20338452/saving-files-downloaded-from-urlretrieve-to-another-folder-other), we will make use of the *os* library to set the full file path (*set_fullpath* function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Defining the destination directory:\n",
    "pathDir = 'BMH_Bulletins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def set_fullpath(directory, url, createDir=False):\n",
    "    ## Getting the file name from the URL using RegEx:\n",
    "    filename = re.findall('(?:[^/]+)$(?<=(?:.jpg)|(?:.pdf))', url)[0]  #The last term is to take the filename out of the list type.\n",
    "        \n",
    "    ## Checking if the directory exists:    \n",
    "    if os.path.isdir(directory):\n",
    "        fullpath = os.path.join(directory, filename)\n",
    "        return fullpath\n",
    "    elif createDir:\n",
    "        os.makedirs(directory)\n",
    "        fullpath = os.path.join(directory, filename)\n",
    "        return fullpath\n",
    "    else:\n",
    "        print('Directory \"{}\" doest not exist.'.format(directory))\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BMH_Bulletins/Dengue/Boletim-epidemiologico-SVS-04fev20.pdf'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_fullpath('BMH_Bulletins/Dengue', url, createDir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.saude.gov.br/images/pdf/2020/fevereiro/04/Boletim-epidemiologico-SVS-04fev20.pdf'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Downloading COVID-19 bulletins:\n",
    "def BMH_download_bulletins(subjectLinks, pathDir, verbose=False, createDir=False):\n",
    "    countSuccess = 0\n",
    "    countFails = 0\n",
    "    for url in subjectLinks:\n",
    "        if verbose:\n",
    "            print(url)\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, set_fullpath(pathDir, url, createDir))\n",
    "            countSuccess += 1\n",
    "            if verbose:\n",
    "                print('Success! \\n')\n",
    "        except:\n",
    "            countFails += 1\n",
    "            if verbose: \n",
    "                print('FAILED! :( \\n')\n",
    "    #if verbose: -- We opt to keep this final message, even when verbose is False.\n",
    "    print('{0} files were successfully downloaded, with {1} fails.'.format(countSuccess, countFails))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.saude.gov.br/images/pdf/2016/fevereiro/02/2015-011---Asbestose.pdf\n",
      "Success! \n",
      "\n",
      "1 files were successfully downloaded, with 0 fails.\n"
     ]
    }
   ],
   "source": [
    "BMH_download_bulletins(links_asbestose,pathDir='BMH_Bulletins/asbestose', verbose=True, createDir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Downloading the files - Version 2.0\n",
    "This version **checks if the file was already downloaded**. In order to allow the code portability, it is recommended to use the own Python libraries whenever it is possible. \n",
    "[Ref.: http://stackoverflow.com/questions/2467609/using-wget-via-python]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using the current version of the urllib library (*request* module) does not allow one to specify the destination folder. Based on [this reference](https://stackoverflow.com/questions/20338452/saving-files-downloaded-from-urlretrieve-to-another-folder-other), we will make use of the *os* library to set the full file path (*set_fullpath* function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Defining the destination directory:\n",
    "pathDir = 'BMH_Bulletins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def set_fullpath(directory, url, createDir=False):\n",
    "    '''Given a local path for a directory and a URL name for a PDF file, \n",
    "    this function defines the full path (directory + filename.pdf) for downloading it. \n",
    "    Syntaxe: \n",
    "        directory: the directory path;\n",
    "        url: the URL containing the PDF file;\n",
    "        createDir: if True, creates the dir in case it doesn't exist. \n",
    "            If false, returns error (-1) if the directory doesn't exist.\n",
    "    '''\n",
    "    ## Getting the file name from the URL using RegEx:\n",
    "    filename = re.findall('(?:[^/]+)$(?<=(?:.jpg)|(?:.pdf))', url)[0]  #The last term is to take the filename out of the list type.\n",
    "        \n",
    "    ## Checking if the directory exists:    \n",
    "    if os.path.isdir(directory):\n",
    "        fullpath = os.path.join(directory, filename)\n",
    "        return fullpath\n",
    "    elif createDir:\n",
    "        os.makedirs(directory)\n",
    "        fullpath = os.path.join(directory, filename)\n",
    "        return fullpath\n",
    "    else:\n",
    "        print('Directory \"{}\" doest not exist.'.format(directory))\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_filenameFromURL(url):\n",
    "    '''From a URL name for a PDF file, gets only the filename.pdf by using regular expression.'''\n",
    "    ## Getting the file name from the URL using RegEx:\n",
    "    filename = re.findall('(?:[^/]+)$(?<=(?:.jpg)|(?:.pdf))', url)[0]  #The last term is to take the filename out of the list type.\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doesFileExists(pathDir, filename, verbose=True):\n",
    "    '''Check whether a file already exists in a given directory.\n",
    "    It returns True, in the case it exists, or False, in the case it doesn't.''' \n",
    "    filelist = os.listdir(pathDir)\n",
    "    if filename in filelist:\n",
    "        if verbose:\n",
    "            print('The file {0} already exists in the specified path.\\n'.format(filename))\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Downloading COVID-19 bulletins:\n",
    "def BMH_download_bulletins_checkExists(subjectLinks, pathDir, verbose=False, createDir=False):\n",
    "    countSuccess = 0\n",
    "    countFails = 0\n",
    "    countExists = 0\n",
    "    for url in subjectLinks:\n",
    "        if verbose:\n",
    "            print(url)\n",
    "        filename = get_filenameFromURL(url)\n",
    "        try:\n",
    "            ## Checking if the file already exists:\n",
    "            if doesFileExists(pathDir, filename, verbose):\n",
    "                countExists += 1\n",
    "            else:\n",
    "                ## Downloading the file:\n",
    "                urllib.request.urlretrieve(url, set_fullpath(pathDir, url, createDir))\n",
    "                countSuccess += 1\n",
    "                if verbose:\n",
    "                    print('Success! \\n')\n",
    "        except:\n",
    "            countFails += 1\n",
    "            if verbose: \n",
    "                print('FAILED! :( \\n')\n",
    "    #if verbose: -- We opt to keep this final message, even when verbose is False.\n",
    "    print('{0} files were successfully downloaded, with {1} fails. {2} already exist.'.format(countSuccess, countFails, countExists))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the functions above by downloading the \"Covid-19\" reprots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files were successfully downloaded, with 1 fails. 15 already exist.\n"
     ]
    }
   ],
   "source": [
    "BMH_download_bulletins_checkExists(links_COVID,pathDir='BMH_Bulletins/covid/', verbose=False, createDir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Concluding with a wrapper function:\n",
    "In this final section we will build a wrapper function with all those pieces of code previously built.  \n",
    "\n",
    "Of course we must take into account that any change in the website that we wish scrape would make our code unfeasible, which requires constant maintenance -- and adaptation.\n",
    "\n",
    "Anyway, as long as the structure of the portal of the Brazilian Ministry of Health remains the same, the function below will remain valid and practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### The webpage must be already collected with chromium and BeautifulSoup. I.e., the \"soup\" must be ready!\n",
    "def BMH_get_bulletins(subject, soup, pathDir='BMH_Bulletins', verbose=True):\n",
    "    links = BMH_findLinks_bySubject_v2(subject, soup)\n",
    "    BMH_download_bulletins_checkExists(links,pathDir, verbose, createDir=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/27/2020-04-27-18-05h-BEE14-Boletim-do-COE.pdf\n",
      "The file 2020-04-27-18-05h-BEE14-Boletim-do-COE.pdf already exists in the specified path.\n",
      "\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/21/BE13---Boletim-do-COE.pdf\n",
      "The file BE13---Boletim-do-COE.pdf already exists in the specified path.\n",
      "\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/19/BE12-Boletim-do-COE.pdf\n",
      "The file BE12-Boletim-do-COE.pdf already exists in the specified path.\n",
      "\n",
      "https://www.saude.gov.br/images/pdf/2020/April/18/2020-04-17---BE11---Boletim-do-COE-21h.pdf\n",
      "The file 2020-04-17---BE11---Boletim-do-COE-21h.pdf already exists in the specified path.\n",
      "\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/17/2020-04-16---BE10---Boletim-do-COE-21h.pdf\n",
      "The file 2020-04-16---BE10---Boletim-do-COE-21h.pdf already exists in the specified path.\n",
      "\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/12/2020-04-11-BE9-Boletim-do-COE.pdf\n",
      "The file 2020-04-11-BE9-Boletim-do-COE.pdf already exists in the specified path.\n",
      "\n",
      "https://www.saude.gov.br/images/pdf/2020/April/09/be-covid-08-final-2.pdf\n",
      "The file be-covid-08-final-2.pdf already exists in the specified path.\n",
      "\n",
      "https://www.saude.gov.br/images/pdf/2020/April/09/be-covid-08-final.pdf\n",
      "The file be-covid-08-final.pdf already exists in the specified path.\n",
      "\n",
      "https://www.saude.gov.br/images/pdf/2020/April/06/2020-04-06-BE7-Boletim-Especial-do-COE-Atualizacao-da-Avaliacao-de-Risco.pdf\n",
      "The file 2020-04-06-BE7-Boletim-Especial-do-COE-Atualizacao-da-Avaliacao-de-Risco.pdf already exists in the specified path.\n",
      "\n",
      "https://portalarquivos.saude.gov.br/images/pdf/2020/April/03/BE6-Boletim-Especial-do-COE.pdf\n",
      "The file BE6-Boletim-Especial-do-COE.pdf already exists in the specified path.\n",
      "\n",
      "https://www.saude.gov.br/images/pdf/2020/marco/21/2020-03-13-Boletim-Epidemiologico-05.pdf\n",
      "FAILED! :( \n",
      "\n",
      "https://www.saude.gov.br/images/pdf/2020/marco/04/2020-03-02-Boletim-Epidemiol--gico-04-corrigido.pdf\n",
      "The file 2020-03-02-Boletim-Epidemiol--gico-04-corrigido.pdf already exists in the specified path.\n",
      "\n",
      "https://www.saude.gov.br/images/pdf/2020/fevereiro/21/2020-02-21-Boletim-Epidemiologico03.pdf\n",
      "The file 2020-02-21-Boletim-Epidemiologico03.pdf already exists in the specified path.\n",
      "\n",
      "https://portalarquivos2.saude.gov.br/images/pdf/2020/fevereiro/13/Boletim-epidemiologico-COEcorona-SVS-13fev20.pdf\n",
      "The file Boletim-epidemiologico-COEcorona-SVS-13fev20.pdf already exists in the specified path.\n",
      "\n",
      "https://www.saude.gov.br/images/pdf/2020/fevereiro/18/Boletim-epidemiologico-SVS-01-COE-inundacao.pdf\n",
      "The file Boletim-epidemiologico-SVS-01-COE-inundacao.pdf already exists in the specified path.\n",
      "\n",
      "https://www.saude.gov.br/images/pdf/2020/fevereiro/04/Boletim-epidemiologico-SVS-04fev20.pdf\n",
      "The file Boletim-epidemiologico-SVS-04fev20.pdf already exists in the specified path.\n",
      "\n",
      "0 files were successfully downloaded, with 1 fails. 15 already exist.\n"
     ]
    }
   ],
   "source": [
    "BMH_get_bulletins('Coronavírus/COVID-19', soup, pathDir='BMH_Bulletins/covid', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 files were successfully downloaded, with 6 fails. 167 already exist.\n"
     ]
    }
   ],
   "source": [
    "BMH_get_bulletins('Dengue', soup, pathDir='BMH_Bulletins/dengue', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files were successfully downloaded, with 6 fails. 172 already exist.\n"
     ]
    }
   ],
   "source": [
    "BMH_get_bulletins('Dengue', soup, pathDir='BMH_Bulletins/dengue', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
